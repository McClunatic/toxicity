{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-20 18:46:26.802095: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    TFAutoModelForSequenceClassification,\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample_submission.csv.zip', 'test.csv.zip', 'test_labels.csv.zip', 'train.csv.zip']\n"
     ]
    }
   ],
   "source": [
    "with zipfile.ZipFile('data/jigsaw-toxic-comment-classification-challenge.zip') as zipf:\n",
    "    print(zipf.namelist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile('data/jigsaw-toxic-comment-classification-challenge.zip') as datazip:\n",
    "    with zipfile.ZipFile(datazip.open('train.csv.zip')) as trainzip:\n",
    "        with trainzip.open('train.csv') as traincsv:\n",
    "            traindf = pd.read_csv(traincsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive:\n",
      "toxic             1.738928\n",
      "severe_toxic     16.674086\n",
      "obscene           3.147730\n",
      "threat           55.638424\n",
      "insult            3.376307\n",
      "identity_hate    18.928944\n",
      "dtype: float64\n",
      "negative:\n",
      "toxic            0.184334\n",
      "severe_toxic     0.168349\n",
      "obscene          0.175985\n",
      "threat           0.167167\n",
      "insult           0.175321\n",
      "identity_hate    0.168147\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "featuredf = traindf.drop(columns='comment_text')\n",
    "\n",
    "positive_weights = (1 / featuredf.sum()) * (len(featuredf) / len(featuredf.columns))\n",
    "negative_weights = (1 / (len(featuredf) - featuredf.sum())) * (len(featuredf) / len(featuredf.columns))\n",
    "\n",
    "print(f'positive:\\n{positive_weights}\\nnegative:\\n{negative_weights}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For brevity, let ``x = logits``, and ``z = labels``. Let\n",
    "``a = positive_weights`` and ``b = negative_weights``, where\n",
    "the weights will cause the model to \"pay extra attention\" (``> 1``) or\n",
    "\"less attention\" (``< 1``) to examples from each class.\n",
    "\n",
    "The weighted logistic loss is:\n",
    "\n",
    "```\n",
    "  a * z * -log(sigmoid(x)) + b * (1 - z) * -log(1 - sigmoid(x))\n",
    "= a * z * -log(1 / (1 + exp(-x))) + b * (1 - z) * -log(exp(-x) / (1 + exp(-x)))\n",
    "= a * z * log(1 + exp(-x)) + b * (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))\n",
    "= a * z * log(1 + exp(-x)) + b * (1 - z) * (x + log(1 + exp(-x))\n",
    "```\n",
    "\n",
    "Rearranging to group ``log`` terms:\n",
    "\n",
    "```\n",
    "  a * z * log(1 + exp(-x)) + b * (1 - z) * (x + log(1 + exp(-x))\n",
    "= b * (1 - z) * x + (a * z + b * (1 - z)) * log(1 + exp(-x))\n",
    "```\n",
    "\n",
    "To simplify expressions, let ``A = a * z`` and ``B = b * (1 - z)``:\n",
    "\n",
    "```\n",
    "  b * (1 - z) * x + (a * z + b * (1 - z)) * log(1 + exp(-x))\n",
    "= B * x + (A + B) * log(1 + exp(-x))\n",
    "```\n",
    "\n",
    "For ``x < 0``, to avoid overflow in ``exp(-x)``, we  can reformulate the\n",
    "above to:\n",
    "\n",
    "```\n",
    "  B * x + (A + B) * log(1 + exp(-x))\n",
    "= B * x + (A + B) * log(exp(-x) * (1 + exp(x)))\n",
    "= B * x + (A + B) * (-x + log(1 + exp(x)))\n",
    "= -A * x + (A + B) * log(1 + exp(x))\n",
    "```\n",
    "\n",
    "So our piecewise stable definition:\n",
    "\n",
    "```\n",
    "=  B * x + (A + B) * log(1 + exp(-x))  # when x > 0\n",
    "= -A * x + (A + B) * log(1 + exp( x))  # when x < 0\n",
    "```\n",
    "\n",
    "Combining the two:\n",
    "\n",
    "```\n",
    "= -A * x + (A + B) * max(x, 0) + (A + B) * log(1 + exp(-abs(x)))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(\n",
    "    labels: tf.Tensor,\n",
    "    logits: tf.Tensor,\n",
    "    positive_weights: np.ndarray = positive_weights,\n",
    "    negative_weights: np.ndarray = negative_weights,\n",
    "):\n",
    "    \"\"\"Class-weighted sigmoid cross entropy given `logits`.\n",
    "\n",
    "    Args:\n",
    "        labels: A :class:`tf.Tensor` of the same type and shape as `logits`.\n",
    "        logits: A :class:`tf.Tensor` of type :class:`tf.float32` or\n",
    "            :class:`tf.float64`. Any real number.\n",
    "        positive_weights: A :class:`np.ndarray` whose size matces the ``-1``\n",
    "            dimension of `logits`. The positive weights for each represented\n",
    "            class label.\n",
    "        negative_weights: A :class:`np.ndarray` whose size matces the ``-1``\n",
    "            dimension of `logits`. The negative weights for each represented\n",
    "            class label.\n",
    "\n",
    "    Returns:\n",
    "        The class-weighted sigmoid cross entropy loss average over classes.         \n",
    "    \"\"\"\n",
    "\n",
    "    labels = tf.cast(labels, logits.dtype)\n",
    "    pos = tf.cast(positive_weights, logits.dtype)\n",
    "    neg = tf.cast(negative_weights, logits.dtype)\n",
    "\n",
    "    zeros = tf.zeros_like(logits)\n",
    "    cond = (logits >= zeros)\n",
    "    relu_logits = tf.where(cond, logits, zeros)\n",
    "    neg_abs_logits = tf.where(cond, -logits, logits)\n",
    "    return tf.reduce_mean((\n",
    "            -pos * labels * logits +\n",
    "            (pos * labels + neg * (1 - labels)) * relu_logits +\n",
    "            (pos * labels + neg * (1 - labels)) * tf.math.log1p(tf.math.exp(neg_abs_logits))\n",
    "        ),\n",
    "        axis=-1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-20 18:46:30.398806: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-12-20 18:46:30.403772: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-12-20 18:46:30.404020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-12-20 18:46:30.405147: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-20 18:46:30.406736: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-12-20 18:46:30.407073: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-12-20 18:46:30.407279: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-12-20 18:46:30.991118: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-12-20 18:46:30.991522: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-12-20 18:46:30.991540: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2022-12-20 18:46:30.991778: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-12-20 18:46:30.991858: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3417 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([1.3132617, 0.9514133], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = np.array([[-1., 1.], [-1.5, 1.5]], dtype=np.float32)\n",
    "labels = np.array([[1, 0], [1, 1]], dtype=np.int64)\n",
    "cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels.astype(logits.dtype), logits)\n",
    "ce_mean = tf.reduce_mean(cross_entropy, -1)\n",
    "ce_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([1.3132617, 0.9514133], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_weights = np.array([1., 1.])\n",
    "loss_fn(labels, logits, default_weights, default_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainds = Dataset.from_pandas(pd.DataFrame(data={\n",
    "    'text': traindf.comment_text,\n",
    "    'label': traindf.drop(columns='comment_text').values.tolist(),\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159571/159571 [00:55<00:00, 2862.90ex/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "def tokenize_dataset(data):\n",
    "    # Keys of the returned dictionary will be added to the dataset as columns\n",
    "    return tokenizer(data['text'], padding='max_length', truncation=True)\n",
    "\n",
    "trainds = trainds.map(tokenize_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-20 18:47:29.329069: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_layer_norm', 'vocab_projector', 'vocab_transform', 'activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier', 'pre_classifier', 'dropout_19']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased',\n",
    "    num_labels=len(traindf.columns) - 1,\n",
    "    problem_type='multi_label_classification',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "tf_trainds = model.prepare_tf_dataset(trainds, batch_size=4, shuffle=True, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(3e-5), loss=loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " distilbert (TFDistilBertMai  multiple                 66362880  \n",
      " nLayer)                                                         \n",
      "                                                                 \n",
      " pre_classifier (Dense)      multiple                  590592    \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  4614      \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 66,958,086\n",
      "Trainable params: 66,958,086\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-20 18:47:38.755871: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x4934c570 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-12-20 18:47:38.755918: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA GeForce RTX 3060 Laptop GPU, Compute Capability 8.6\n",
      "2022-12-20 18:47:38.761973: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-12-20 18:47:38.864769: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39892/39892 [==============================] - 7458s 187ms/step - loss: 0.0770\n",
      "Epoch 2/3\n",
      "39892/39892 [==============================] - 6691s 168ms/step - loss: 0.0779\n",
      "Epoch 3/3\n",
      "39892/39892 [==============================] - 6560s 164ms/step - loss: 0.0717\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7faab803c1c0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(tf_trainds, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('toxicity-4batch-3epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Jun 22 2022, 20:18:18) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
